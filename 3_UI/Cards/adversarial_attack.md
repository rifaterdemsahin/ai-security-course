```markdown
- question : ðŸŽ¯ adversarial attack
- hint : Think about inputs that look normal to humans but confuse AI models
- answer : A malicious input designed to fool AI models by adding imperceptible perturbations that cause misclassification while appearing normal to humans
```