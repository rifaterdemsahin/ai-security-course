```markdown
- question : ðŸŽ¯ adversarial attack
- hint : Think about inputs that look normal to humans but confuse AI models
- answer : A malicious input designed to fool AI models by adding imperceptible perturbations that cause misclassification while appearing normal to humans
- reference : <a href="https://www.youtube.com/watch?v=p_i32sJc2-A" target="_blank">This Tiny Change BREAKS AI | FGSM Adversarial Attack Explained</a>
```