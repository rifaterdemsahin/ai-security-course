- question : ðŸŽ¯ adversarial attack
- hint : Think about inputs that look normal to humans but confuse AI models
- answer : A malicious input designed to fool AI models by adding imperceptible perturbations that cause misclassification while appearing normal to humans
- youtubereference : <a href="https://www.youtube.com/watch?v=p_i32sJc2-A" target="_blank">This Tiny Change BREAKS AI | FGSM Adversarial Attack Explained</a>
- googleimages : <a href="https://www.google.com/search?q=adversarial+attack+AI+security+examples&tbm=isch" target="_blank">Adversarial Attack Examples and Visualizations</a>