- question : ðŸ”´ red teaming
- hint : Think about simulating adversarial attacks to test system security
- answer : A systematic security testing methodology where a team simulates adversarial attacks to identify vulnerabilities and test the effectiveness of defensive measures in AI systems
- reference : <a href="https://www.youtube.com/watch?v=vV4KJQ2rOe4" target="_blank">AI Red Teaming: Testing AI Security</a>