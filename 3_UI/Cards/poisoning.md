- question : ☠️ poisoning attack
- hint : Think about compromising the training data to create hidden backdoors
- answer : Attacks that modify training data to add a backdoor. For example, imperceptible patterns in training data create backdoors that control models.
- youtubereference : <a href="https://www.youtube.com/results?search_query=data+poisoning+attack+AI+security" target="_blank">Data Poisoning Attack AI Security Videos</a>
- googleimages : <a href="https://www.google.com/search?q=data+poisoning+attack+AI+security+examples&tbm=isch" target="_blank">Data Poisoning Attack Examples and Visualizations</a>
- readingreferences : <a href="https://www.google.com/search?q=data+poisoning+attack+AI+security+research+papers" target="_blank">Data Poisoning Attack Research Papers and Articles</a>
