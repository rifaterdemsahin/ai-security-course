"""
Lesson 1: AI Vulnerability Assessment Tool
==========================================

This script provides a comprehensive framework for assessing AI system vulnerabilities.
It demonstrates how to think like an attacker and systematically evaluate security risks.

Learning Objectives:
- Learn to systematically assess AI vulnerabilities
- Understand how to document security risks
- Practice thinking from an attacker's perspective
- Develop a security-first mindset for AI systems
"""

import json
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional

class AttackCategory(Enum):
    """Categories of AI attacks"""
    EVASION = "evasion"
    POISONING = "poisoning"
    EXTRACTION = "extraction"
    INFERENCE = "inference"
    OTHER = "other"

class RiskLevel(Enum):
    """Risk severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class AttackVector(Enum):
    """Different ways attacks can be executed"""
    INPUT_MANIPULATION = "input_manipulation"
    TRAINING_DATA = "training_data"
    MODEL_QUERIES = "model_queries"
    SIDE_CHANNELS = "side_channels"
    PHYSICAL_ACCESS = "physical_access"

@dataclass
class Vulnerability:
    """Represents a security vulnerability in an AI system"""
    id: str
    title: str
    description: str
    category: AttackCategory
    risk_level: RiskLevel
    attack_vectors: List[AttackVector]
    impact: str
    likelihood: str
    affected_components: List[str]
    mitigation_strategies: List[str]
    references: List[str]
    discovered_date: str

class AIVulnerabilityAssessment:
    """Comprehensive AI security vulnerability assessment tool"""
    
    def __init__(self, system_name: str = "AI System"):
        self.system_name = system_name
        self.vulnerabilities: List[Vulnerability] = []
        self.assessment_date = datetime.now().isoformat()
        self.assessment_metadata = {
            "version": "1.0",
            "assessor": "AI Security Course Student",
            "methodology": "OWASP AI Security Guidelines + Custom Framework"
        }
    
    def add_vulnerability(self, vulnerability: Vulnerability):
        """Add a vulnerability to the assessment"""
        self.vulnerabilities.append(vulnerability)
        print(f"â• Added vulnerability: {vulnerability.title}")
    
    def assess_evasion_vulnerabilities(self):
        """Assess evasion attack vulnerabilities"""
        print("\nğŸ¯ Assessing Evasion Attack Vulnerabilities...")
        
        # Adversarial example vulnerability
        vuln1 = Vulnerability(
            id="EVAS-001",
            title="Adversarial Example Susceptibility",
            description="Model vulnerable to small input perturbations that cause misclassification",
            category=AttackCategory.EVASION,
            risk_level=RiskLevel.HIGH,
            attack_vectors=[AttackVector.INPUT_MANIPULATION],
            impact="Incorrect predictions on manipulated inputs, potential safety failures",
            likelihood="High - easily exploitable with gradient-based methods",
            affected_components=["Input preprocessing", "Model inference"],
            mitigation_strategies=[
                "Adversarial training",
                "Input validation and sanitization",
                "Ensemble methods",
                "Defensive distillation"
            ],
            references=[
                "Goodfellow et al. (2014) - Explaining and Harnessing Adversarial Examples",
                "Madry et al. (2017) - Towards Deep Learning Models Resistant to Adversarial Attacks"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        # Model overconfidence vulnerability
        vuln2 = Vulnerability(
            id="EVAS-002",
            title="Model Overconfidence on OOD Inputs",
            description="Model produces confident predictions on out-of-distribution inputs",
            category=AttackCategory.EVASION,
            risk_level=RiskLevel.MEDIUM,
            attack_vectors=[AttackVector.INPUT_MANIPULATION],
            impact="False confidence in predictions, potential for exploitation",
            likelihood="Medium - requires understanding of training distribution",
            affected_components=["Model output layer", "Confidence estimation"],
            mitigation_strategies=[
                "Uncertainty quantification",
                "Out-of-distribution detection",
                "Temperature scaling",
                "Confidence thresholding"
            ],
            references=[
                "Hendrycks & Gimpel (2016) - A Baseline for Detecting Misclassified and OOD Examples"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        self.add_vulnerability(vuln1)
        self.add_vulnerability(vuln2)
    
    def assess_poisoning_vulnerabilities(self):
        """Assess data poisoning vulnerabilities"""
        print("\nğŸ’‰ Assessing Data Poisoning Vulnerabilities...")
        
        # Training data poisoning
        vuln1 = Vulnerability(
            id="POIS-001",
            title="Training Data Poisoning",
            description="Malicious samples in training data can create backdoors or degrade performance",
            category=AttackCategory.POISONING,
            risk_level=RiskLevel.CRITICAL,
            attack_vectors=[AttackVector.TRAINING_DATA],
            impact="Backdoor functionality, degraded performance, compromised model integrity",
            likelihood="Medium - requires access to training pipeline",
            affected_components=["Training data", "Model weights", "Training pipeline"],
            mitigation_strategies=[
                "Data provenance tracking",
                "Anomaly detection in training data",
                "Robust training algorithms",
                "Data validation and sanitization"
            ],
            references=[
                "Gu et al. (2017) - BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        # Model supply chain vulnerability
        vuln2 = Vulnerability(
            id="POIS-002",
            title="Model Supply Chain Compromise",
            description="Pre-trained models or components may contain hidden backdoors",
            category=AttackCategory.POISONING,
            risk_level=RiskLevel.HIGH,
            attack_vectors=[AttackVector.TRAINING_DATA],
            impact="Inherited vulnerabilities, unknown backdoors, compromised foundation",
            likelihood="Low - but high impact if occurs",
            affected_components=["Pre-trained models", "Model libraries", "Dependencies"],
            mitigation_strategies=[
                "Model validation and testing",
                "Trusted model sources",
                "Model fingerprinting",
                "Behavioral analysis"
            ],
            references=[
                "Ji et al. (2017) - Model-Reuse Attacks on Deep Learning Systems"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        self.add_vulnerability(vuln1)
        self.add_vulnerability(vuln2)
    
    def assess_extraction_vulnerabilities(self):
        """Assess model extraction vulnerabilities"""
        print("\nğŸ•µï¸ Assessing Model Extraction Vulnerabilities...")
        
        # API-based extraction
        vuln1 = Vulnerability(
            id="EXTR-001",
            title="API Query-Based Model Extraction",
            description="Attackers can steal model functionality through systematic API queries",
            category=AttackCategory.EXTRACTION,
            risk_level=RiskLevel.HIGH,
            attack_vectors=[AttackVector.MODEL_QUERIES],
            impact="Intellectual property theft, competitive advantage loss, model replication",
            likelihood="High - only requires API access",
            affected_components=["API endpoints", "Model inference service"],
            mitigation_strategies=[
                "Query rate limiting",
                "Query pattern monitoring",
                "Output perturbation/noise",
                "Authentication and audit logging"
            ],
            references=[
                "TramÃ¨r et al. (2016) - Stealing Machine Learning Models via Prediction APIs"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        # Model inversion
        vuln2 = Vulnerability(
            id="EXTR-002",
            title="Model Inversion Attacks",
            description="Attackers can infer sensitive training data from model outputs",
            category=AttackCategory.EXTRACTION,
            risk_level=RiskLevel.MEDIUM,
            attack_vectors=[AttackVector.MODEL_QUERIES, AttackVector.SIDE_CHANNELS],
            impact="Privacy breach, training data exposure, sensitive information leakage",
            likelihood="Medium - requires sophisticated techniques",
            affected_components=["Model outputs", "Gradient information"],
            mitigation_strategies=[
                "Differential privacy",
                "Output masking",
                "Gradient clipping",
                "Limited precision outputs"
            ],
            references=[
                "Fredrikson et al. (2015) - Model Inversion Attacks that Exploit Confidence Information"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        self.add_vulnerability(vuln1)
        self.add_vulnerability(vuln2)
    
    def assess_inference_vulnerabilities(self):
        """Assess membership inference vulnerabilities"""
        print("\nğŸ” Assessing Inference Attack Vulnerabilities...")
        
        # Membership inference
        vuln1 = Vulnerability(
            id="INFER-001",
            title="Membership Inference Attacks",
            description="Attackers can determine if specific data was used in training",
            category=AttackCategory.INFERENCE,
            risk_level=RiskLevel.MEDIUM,
            attack_vectors=[AttackVector.MODEL_QUERIES],
            impact="Privacy violation, training data exposure, regulatory compliance issues",
            likelihood="Medium - requires shadow training",
            affected_components=["Model confidence scores", "Prediction behavior"],
            mitigation_strategies=[
                "Differential privacy training",
                "Regularization techniques",
                "Confidence score masking",
                "Data minimization"
            ],
            references=[
                "Shokri et al. (2017) - Membership Inference Attacks against Machine Learning Models"
            ],
            discovered_date=datetime.now().isoformat()
        )
        
        self.add_vulnerability(vuln1)
    
    def perform_comprehensive_assessment(self):
        """Perform a comprehensive vulnerability assessment"""
        print(f"ğŸ›¡ï¸  Starting comprehensive assessment for: {self.system_name}")
        print("=" * 60)
        
        # Assess different vulnerability categories
        self.assess_evasion_vulnerabilities()
        self.assess_poisoning_vulnerabilities()
        self.assess_extraction_vulnerabilities()
        self.assess_inference_vulnerabilities()
        
        print(f"\nâœ… Assessment complete! Found {len(self.vulnerabilities)} vulnerabilities.")
    
    def generate_risk_summary(self):
        """Generate a risk summary of all vulnerabilities"""
        print("\nğŸ“Š Risk Summary")
        print("-" * 30)
        
        # Count vulnerabilities by risk level
        risk_counts = {}
        for risk in RiskLevel:
            count = sum(1 for v in self.vulnerabilities if v.risk_level == risk)
            risk_counts[risk.value] = count
        
        # Display summary
        for risk_level, count in risk_counts.items():
            if count > 0:
                emoji = {
                    'critical': 'ğŸ”´',
                    'high': 'ğŸŸ ',
                    'medium': 'ğŸŸ¡',
                    'low': 'ğŸŸ¢',
                    'info': 'ğŸ”µ'
                }.get(risk_level, 'âšª')
                print(f"{emoji} {risk_level.upper()}: {count} vulnerabilities")
        
        # Calculate overall risk score
        risk_weights = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1, 'info': 0}
        total_score = sum(risk_weights.get(v.risk_level.value, 0) for v in self.vulnerabilities)
        max_score = len(self.vulnerabilities) * 4
        
        if max_score > 0:
            risk_percentage = (total_score / max_score) * 100
            print(f"\nğŸ“ˆ Overall Risk Score: {risk_percentage:.1f}%")
            
            if risk_percentage >= 75:
                print("ğŸš¨ CRITICAL: Immediate action required!")
            elif risk_percentage >= 50:
                print("âš ï¸  HIGH: Significant security concerns")
            elif risk_percentage >= 25:
                print("ğŸ”¶ MEDIUM: Some security improvements needed")
            else:
                print("âœ… LOW: Relatively secure with minor issues")
        
        return risk_counts
    
    def generate_detailed_report(self):
        """Generate detailed vulnerability report"""
        print("\nğŸ“‹ Detailed Vulnerability Report")
        print("=" * 50)
        
        for i, vuln in enumerate(self.vulnerabilities, 1):
            risk_emoji = {
                RiskLevel.CRITICAL: 'ğŸ”´',
                RiskLevel.HIGH: 'ğŸŸ ',
                RiskLevel.MEDIUM: 'ğŸŸ¡',
                RiskLevel.LOW: 'ğŸŸ¢',
                RiskLevel.INFO: 'ğŸ”µ'
            }.get(vuln.risk_level, 'âšª')
            
            print(f"\n{i}. {risk_emoji} {vuln.title} ({vuln.id})")
            print(f"   Category: {vuln.category.value.title()}")
            print(f"   Risk Level: {vuln.risk_level.value.title()}")
            print(f"   Description: {vuln.description}")
            print(f"   Impact: {vuln.impact}")
            print(f"   Likelihood: {vuln.likelihood}")
            print(f"   Affected Components: {', '.join(vuln.affected_components)}")
            print(f"   Top Mitigations:")
            for j, mitigation in enumerate(vuln.mitigation_strategies[:3], 1):
                print(f"      {j}. {mitigation}")
            if len(vuln.mitigation_strategies) > 3:
                print(f"      ... and {len(vuln.mitigation_strategies) - 3} more")
    
    def export_to_json(self, filename: str = None):
        """Export assessment to JSON file"""
        if filename is None:
            filename = f"/Users/rifaterdemsahin/projects/ai-security-course/5_Symbols/Lessons/Lesson 1/vulnerability_assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Convert to serializable format
        export_data = {
            "system_name": self.system_name,
            "assessment_date": self.assessment_date,
            "metadata": self.assessment_metadata,
            "vulnerabilities": [
                {
                    **asdict(vuln),
                    "category": vuln.category.value,
                    "risk_level": vuln.risk_level.value,
                    "attack_vectors": [av.value for av in vuln.attack_vectors]
                }
                for vuln in self.vulnerabilities
            ]
        }
        
        try:
            with open(filename, 'w') as f:
                json.dump(export_data, f, indent=2)
            print(f"\nğŸ’¾ Assessment exported to: {filename}")
        except Exception as e:
            print(f"\nâŒ Failed to export assessment: {e}")
        
        return filename
    
    def generate_mitigation_priority_list(self):
        """Generate prioritized list of mitigations"""
        print("\nğŸ¯ Priority Mitigation Recommendations")
        print("-" * 40)
        
        # Group by risk level and collect unique mitigations
        mitigation_priority = {}
        
        for vuln in self.vulnerabilities:
            risk_level = vuln.risk_level.value
            if risk_level not in mitigation_priority:
                mitigation_priority[risk_level] = set()
            
            for mitigation in vuln.mitigation_strategies:
                mitigation_priority[risk_level].add(mitigation)
        
        # Display in priority order
        priority_order = ['critical', 'high', 'medium', 'low', 'info']
        
        for risk_level in priority_order:
            if risk_level in mitigation_priority and mitigation_priority[risk_level]:
                emoji = {
                    'critical': 'ğŸ”´',
                    'high': 'ğŸŸ ',
                    'medium': 'ğŸŸ¡',
                    'low': 'ğŸŸ¢',
                    'info': 'ğŸ”µ'
                }.get(risk_level, 'âšª')
                
                print(f"\n{emoji} {risk_level.upper()} Priority:")
                for mitigation in sorted(mitigation_priority[risk_level]):
                    print(f"   â€¢ {mitigation}")

def main():
    """Main demonstration function"""
    print("ğŸ›¡ï¸  AI Security Course - Lesson 1: Vulnerability Assessment")
    print("=" * 60)
    print("This tool demonstrates systematic AI security vulnerability assessment.")
    
    # Create assessment for a sample system
    assessment = AIVulnerabilityAssessment("Sample AI Image Classifier")
    
    # Perform comprehensive assessment
    assessment.perform_comprehensive_assessment()
    
    # Generate reports
    assessment.generate_risk_summary()
    assessment.generate_detailed_report()
    assessment.generate_mitigation_priority_list()
    
    # Export results
    assessment.export_to_json()
    
    print("\nğŸ“ Key Assessment Principles:")
    print("   1. Think like an attacker - consider all attack vectors")
    print("   2. Assess impact AND likelihood for proper risk rating")
    print("   3. Document everything - vulnerabilities, mitigations, references")
    print("   4. Prioritize based on risk level and business impact")
    print("   5. Create actionable mitigation strategies")
    print("   6. Regular reassessment as threats evolve")
    
    print("\nğŸ”§ Next Steps:")
    print("   â€¢ Review each vulnerability in detail")
    print("   â€¢ Implement high-priority mitigations first")
    print("   â€¢ Set up monitoring for attack indicators")
    print("   â€¢ Plan regular security assessments")
    print("   â€¢ Train team on AI security best practices")

if __name__ == "__main__":
    main()