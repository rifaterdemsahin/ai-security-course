<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lesson 1: The Attacker's Playbook - Understanding AI Vulnerabilities</title>
    <link rel="stylesheet" href="../../style.css">
    <style>
        .lesson-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .lesson-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 30px;
        }
        
        .section {
            background: white;
            margin: 20px 0;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            border-left: 5px solid #667eea;
        }
        
        .section h2 {
            color: #667eea;
            margin-top: 0;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .section-icon {
            font-size: 1.5em;
        }
        
        .objectives {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        
        .learning-items {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .learning-item {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #28a745;
        }
        
        .duration {
            background: #e9ecef;
            padding: 10px;
            border-radius: 5px;
            display: inline-block;
            margin: 10px 0;
        }
        
        .attack-type {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .critical-understanding {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="lesson-container">
        <div class="lesson-header">
            <h1>Lesson 1: The Attacker's Playbook</h1>
            <p>Understanding AI Vulnerabilities</p>
            <div class="duration">Duration: 60 minutes | Learning Objective: LO1</div>
        </div>

        <!-- TELL Section -->
        <div class="section">
            <h2><span class="section-icon">üìö</span>Tell: Understanding the Concepts</h2>
            
            <div class="objectives">
                <h3>Learning Objectives</h3>
                <p>By the end of this lesson, you will be able to:</p>
                <ul>
                    <li>Analyze and identify security vulnerabilities in AI models</li>
                    <li>Understand the three primary attack categories: evasion, data poisoning, and model extraction</li>
                    <li>Develop an adversarial mindset for security analysis</li>
                    <li>Recognize the difference between test accuracy and adversarial robustness</li>
                </ul>
            </div>

            <h3>Core Concepts</h3>
            
            <div class="attack-type">
                <h4>üéØ Evasion Attacks - Fooling the Model's Senses</h4>
                <p><strong>The Accuracy Paradox:</strong> A model with 99% accuracy can still be dangerously insecure.</p>
                <ul>
                    <li>Adversaries craft inputs that look normal but break model logic</li>
                    <li>Difference between test accuracy and adversarial robustness</li>
                    <li>Example: Stop sign stickers that fool self-driving cars</li>
                </ul>
            </div>

            <div class="attack-type">
                <h4>üîç Data Poisoning - Corrupting Intelligence from Within</h4>
                <p><strong>The Sleeper Agent Attack:</strong> AI models can be turned into "sleeper agents" without detection.</p>
                <ul>
                    <li>Backdoors embedded during the training phase</li>
                    <li>Poisoned models appear to function perfectly</li>
                    <li>Specific inputs activate malicious behavior</li>
                    <li>Training-time vs. test-time attacks</li>
                </ul>
            </div>

            <div class="attack-type">
                <h4>üíª Model Stealing and Extraction - The Digital Heist</h4>
                <p><strong>The Invisible Theft:</strong> Stealing AI's 'secret sauce' without touching the code.</p>
                <ul>
                    <li>Query-based attacks through API calls</li>
                    <li>Model extraction with black-box access</li>
                    <li>Recognizing extraction attempts</li>
                    <li>Intellectual property theft concerns</li>
                </ul>
            </div>

            <div class="critical-understanding">
                <h4>üéØ Critical Understanding</h4>
                <p>AI systems are not infallible black boxes; they are software with unique vulnerabilities. You must learn to see models through the eyes of an attacker, recognizing how they can be manipulated through various attack vectors.</p>
            </div>
        </div>

        <!-- SHOW Section -->
        <div class="section">
            <h2><span class="section-icon">üëÅÔ∏è</span>Show: Real-World Examples and Demonstrations</h2>
            
            <div class="learning-items">
                <div class="learning-item">
                    <h4>üöó Stop Sign Attack on Self-Driving Cars</h4>
                    <p>Researchers demonstrated how simple stickers on a stop sign can trick image recognition systems into misclassifying it as a speed limit sign.</p>
                    <div class="duration">Impact: Catastrophic model failure in safety-critical systems</div>
                </div>

                <div class="learning-item">
                    <h4>üõ°Ô∏è Coach Dialogue: The Trojan Horse in the Code</h4>
                    <p>Interactive scenario: You're a lead ML Engineer whose AI-powered malware detector failed to detect a novel ransomware variant, resulting in a client breach.</p>
                    <div class="duration">Duration: 10-15 minutes</div>
                </div>

                <div class="learning-item">
                    <h4>üìπ Video Demonstrations</h4>
                    <ul>
                        <li>Evasion attacks in action (7.5 min)</li>
                        <li>Data poisoning backdoor activation (7.5 min)</li>
                        <li>Model extraction through API queries (7.5 min)</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üîç Case Study Analysis</h4>
                    <p>Examine real incidents where AI systems were compromised, analyzing the attack vectors and impact on organizations.</p>
                </div>
            </div>
        </div>

        <!-- DO Section -->
        <div class="section">
            <h2><span class="section-icon">üõ†Ô∏è</span>Do: Hands-On Activities</h2>
            
            <div class="learning-items">
                <div class="learning-item">
                    <h4>üéØ Attack Simulation Lab</h4>
                    <p>Practice crafting adversarial examples against a sample image classifier:</p>
                    <ul>
                        <li>Generate evasion attacks using gradient-based methods</li>
                        <li>Observe how small perturbations fool the model</li>
                        <li>Measure attack success rates</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üïµÔ∏è Vulnerability Assessment</h4>
                    <p>Analyze a provided AI model for potential weaknesses:</p>
                    <ul>
                        <li>Identify potential attack surfaces</li>
                        <li>Document vulnerability types</li>
                        <li>Assess risk levels</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üìä Attack Pattern Recognition</h4>
                    <p>Study datasets containing normal and adversarial examples:</p>
                    <ul>
                        <li>Learn to spot manipulation patterns</li>
                        <li>Practice visual inspection techniques</li>
                        <li>Use statistical analysis tools</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üî¨ Incident Response Simulation</h4>
                    <p>Role-play responding to a security breach:</p>
                    <ul>
                        <li>Investigate the attack vector</li>
                        <li>Assess damage and scope</li>
                        <li>Propose remediation steps</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- APPLY Section -->
        <div class="section">
            <h2><span class="section-icon">üöÄ</span>Apply: Real-World Application</h2>
            
            <div class="objectives">
                <h3>Capstone Project: Security Assessment Report</h3>
                <p>Create a comprehensive security assessment for a real or simulated AI system in your domain of interest.</p>
            </div>

            <div class="learning-items">
                <div class="learning-item">
                    <h4>üìã Assessment Framework</h4>
                    <p>Develop a systematic approach to evaluate AI security:</p>
                    <ul>
                        <li>Define threat models for your chosen domain</li>
                        <li>Create vulnerability assessment checklist</li>
                        <li>Establish risk scoring methodology</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üéØ Target System Analysis</h4>
                    <p>Choose an AI application and perform security analysis:</p>
                    <ul>
                        <li>Healthcare: Medical imaging diagnosis</li>
                        <li>Finance: Fraud detection systems</li>
                        <li>Automotive: Autonomous driving</li>
                        <li>Security: Malware detection</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üìñ Professional Report</h4>
                    <p>Document your findings in a professional security assessment:</p>
                    <ul>
                        <li>Executive summary with key risks</li>
                        <li>Technical analysis of vulnerabilities</li>
                        <li>Risk prioritization matrix</li>
                        <li>Recommended mitigation strategies</li>
                    </ul>
                </div>

                <div class="learning-item">
                    <h4>üé§ Presentation & Peer Review</h4>
                    <p>Present your assessment to peers for feedback:</p>
                    <ul>
                        <li>10-minute presentation of key findings</li>
                        <li>Q&A session with classmates</li>
                        <li>Peer evaluation of assessment quality</li>
                        <li>Incorporate feedback for final submission</li>
                    </ul>
                </div>
            </div>

            <div class="critical-understanding">
                <h4>üéØ Real-World Connection</h4>
                <p>This assessment mirrors what security professionals do in practice. You're building skills that directly translate to protecting AI systems in production environments.</p>
            </div>
        </div>

        <div class="section">
            <h2>üìà Progress Check</h2>
            <p>Before moving to Lesson 2, ensure you can:</p>
            <ul>
                <li>‚úÖ Identify the three main categories of AI attacks</li>
                <li>‚úÖ Explain why high test accuracy doesn't guarantee security</li>
                <li>‚úÖ Recognize potential attack vectors in AI systems</li>
                <li>‚úÖ Think from an attacker's perspective about AI vulnerabilities</li>
            </ul>
        </div>
    </div>
</body>
</html>