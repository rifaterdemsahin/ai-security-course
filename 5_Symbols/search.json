{
  "modules": [
    {
      "title": "Module 1: The Attacker's Playbook",
      "description": "Understanding AI Vulnerabilities",
      "keywords": ["attack", "vulnerability", "adversarial"],
      "duration": "21 minutes",
      "objective": "LO1"
    }
  ],
  "lessons": [
    {
      "title": "Lesson 1: The Attacker's Playbook",
      "subtitle": "Understanding AI Vulnerabilities",
      "concepts": ["evasion", "data poisoning", "model extraction"],
      "duration": "21 minutes",
      "objective": "LO1",
      "file_path": "5_Symbols/Lessons/Lesson 1/index.html"
    }
  ],
  "technical_terms": [
    {
      "term": "Adversarial Attack",
      "definition": "A technique used to fool a machine learning model with malicious input.",
      "emoji": "ðŸŽ¯",
      "card_file": "3_UI/Cards/adversarial_attack.md"
    }
  ],
  "learning_objectives": [
    {
      "id": "LO1",
      "description": "Analyze and identify security vulnerabilities in AI models."
    }
  ],
  "content_files": [
    {
      "title": "README.md",
      "type": "Documentation",
      "keywords": ["project", "setup", "guide"],
      "file_path": "README.md"
    }
  ],
  "hands_on_labs": [
    {
      "title": "Attack Simulation Lab",
      "description": "Practice crafting adversarial examples against a sample image classifier.",
      "duration": "15 minutes",
      "module": "Module 1",
      "objective": "LO1"
    }
  ],
  "key_concepts": [
    "Adversarial Robustness",
    "Differential Privacy",
    "Red Teaming"
  ],
  "search_index": [
    {
      "file_path": "4_formulas/outline/module1.md",
      "line_number": 5,
      "content": "**Duration:** Approximately 21 minutes",
      "keywords": ["duration", "module 1"]
    }
  ]
}