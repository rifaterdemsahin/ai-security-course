# Technical Terms

- Adversarial AI landscape
- Adversarial Attack Evasion Example
- Adversarial Dataset Augmentation
- Adversarial Example Generation
- Adversarial Examples
- Adversarial Patches
- Adversarial Robustness Toolbox (ART)
- Adversarial Testing
- Adversarial Training
- Adversarial Training Workflow
- AI
- AI Incident Response
- AI Model Robustness Evaluation
- AI Model Robustness Techniques
- AI Model Threat Model
- AI Red Teaming Framework
- AI Security
- AI Security Lifecycle
- AI Security Mind Map
- AI Supply Chain Attack
- Backdoor Attack Machine Learning
- Data Poisoning
- Data Poisoning Attack Diagram
- Detecting Model Extraction Attacks
- Differential Privacy
- Differential Privacy Graph
- Evasion Attacks
- Executing Adversarial Attacks
- Federated Learning Security
- FGSM
- Google Colab
- Input Sanitization
- Jupyter Notebook
- Large Language Model (LLM)
- Machine Learning Security Vulnerabilities
- Model Extraction
- Model Extraction Attack Schematic
- Model Hardening Security
- Model Retraining For Resilience
- Model Stealing Attack AI
- Penetration Testing AI Systems
- PGD
- PyTorch
- Robust AI System Architecture
- Robustness Evaluation
- Security
- Security Evaluation And Reporting
- Security Vs Accuracy Tradeoff Graph
- TensorFlow
